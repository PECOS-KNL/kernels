% DNS defined in the intro!
\subsection{Incompressible DNS Application}
\label{sec:dns_full}

% Why DNS code needs to be improved
Direct numerical simulation (DNS) plays an important role in understanding turbulent flows because DNS provides high fidelity data that is difficult to obtain in the experiments. After Kim {\it et al.} first used the DNS for wall-bounded turbulence flow in 1987 \cite{Kim:1987ub}, DNS has been used extensively to understand the turbulence phenomenon and to help develop models of turbulence. In general, the length-scale and time-scale become smaller as $Re$ increases where $Re$ is Reynolds number. Naturally, DNS at high $Re$ requires a large number of grids and time-steps to obtain meaningful statistical data. Therefore, the use of state-of-art HPC system is crucial to study the turbulent flow of high Re. To date, the highest $Re$ of the wall-bounded turbulence DNS is 250000, for which 242 billion degrees of freedom are used. \cite{Lee:2015er} However, $Re$ = 250000 is also low compared to the high Re flow required for many practical engineering applications. For a DNS at higher Re, more advanced HPC system is required, and the DNS code should also be improved for the the HPC system.

% Detail of simulation
In this section, we will show you the results of testing PoongBack on KNL nodes in Stampede, which is a channel flow DNS code optimized for the modern HPC systems. PoongBack simulates the flow between two parallel plates with periodic boundary conditions. The simulation code has already shown excellent performance and has been already used in several researchers. \cite{Lee:2013kv} Especially, it has been used for generating the data for virtual flow laboratory in Johns Hopkins Turbulence Data Base \cite{Graham:2015ha}.It is using Fourier-Galerkin method in streamwise direction and high order basis spline method in wall-normal directions. For time integration PoongBack uses low-storage third order Runge-Kutta method. The simulation domain is partitioned by two-dimensional decomposition, a.k.a. pencil-decomposition. PoongBack contains three major kernels; (1) solving Navier-Stokes equation in complex number domain (2) one-dimensional fast Fourier transforms (3) data transpose in two dimension. The combination of (2) and (3) makes 3D FFT and there are multiple libraries for it. We developed customized 3D FFT library because of the needs of zero-padding for 3/2 dealiasing. Additionally, PoongBack uses customized I/O library for HDF5 format, ESIO \cite{Lee:2014ta}, but the I/O performance is not the scope of current work. See \cite{Lee:2013kv,Lee:2014ta} for more detail about PoongBack.  

For this study the grid size is used $1024\times128\times512$ and the grid size is comparable for $Re_\tau = 180$ simulation by \cite{Kim:1987ub}. Throughout the every benchmark cases, the MCDRAM is used as a cache memory between processors and DRAM. The simulation code is compiled with a flag ``{\tt -xMIC-AVX512}.'' Also, the FFTW-3.3.5 library is installed with the options ``{\tt --enable-avx512}'' and ``{\tt --enable-mpi}.'' \cite{Frigo:2005tu} We used double-precision operation for all kernels and elapsed times were measured by ``{\tt mpi\_wtime()}'' with appropriate ``{\tt mpi\_barriers()}.''

\begin{figure}[htb]
 \begin{center}
   \includegraphics[width=0.45\textwidth]{DNS_FFT_Wave}
   \caption{Strong scaling result of 1D FFTs and Solving N-S equations in wavespace for single timestep.}
   \label{fig:DNS_strong_scale_fft_wave}
 \end{center}
\end{figure}

% 1D FFT and wavespace performance
Figure~\ref{fig:DNS_strong_scale_fft_wave} shows the strong scaling performance of the kernels involving floating point operations; 1D FFTs and solving Navier-Stokes equation. Four different FFTs are performed; real-to-complex (R2C), complex-to-real (C2R), forward complex-to-complex (fC2C) and backward complex-to-complex (bC2C). Before or after each FFTs zeroes in the half length of 1d contiguous data lines are padded or truncated. Also, nonlinear products such as $A\times B \rightarrow C$ are performed between C2R and R2C transforms. The elapsed time for 1d FFTs, zero padding/truncation and nonlinear product is shown in figure~\ref{fig:DNS_strong_scale_fft_wave} as FFT. In both case of using MPI only and OpenMP only the performance is almost linear up to 64 processors are used. When two hardware threads were used per core, the performance did not increased. Also, the performance decreases when four hardware threads were used in both MPI and OpenMP cases. The performance was the best with 64 processors and showed 270 GFlops which is approximately 9\% of peak performance. It means that the performance of 1D FFTs on KNL nodes are memory bandwidth bounded. There are numerous floating point operation involved in the kernel of solving Navier-Stokes equation. Solving linear equations $A \textbf{x} = \textbf{b}$ and matrix-vector multiplications are the most important parts in the kernel. Here, the matrix $A$ is in a form of banded matrix with additional non-zero elements in several first and last rows. Also, the elements of $A$ are real whereas the elements of $\textbf{x}$ and $\textbf{b}$ are complex. Because of these properties, we have implemented customized linear algebra solver. Since the Navier-Stokes equation is solved in complex number domain after 1d FFTs in two directions, 3D PDE becomes 1D ODE at each wavenumbers. Hence, the Navier-Stokes equation can be solved at each wavenumbers without interaction with the data in other wavenumbers. As a result, the kernel can be parallelized and requires no communication. The performance of the kernel of solving Navier-Stokes equation is shown in figure~\ref{fig:DNS_strong_scale_fft_wave} as Solve N-S eq. Similar to FFT, both MPI only and OpenMP only cases shows almost ideal scalability up to 64 cores. There were small, but noticeable, performance increases observed with two hardware threads. Interestingly, the performance of OpenMP only case decreases while the performance of MPI only case increases when four hardware threads per core were applied. This maybe due to the difference of memory access pattern between two parallelism or scheduling overhead in OpenMP. \todo{MK:Please tell me if this explanation is reasonable.}         

% Transpose performance
\begin{figure}[htb]
 \begin{center}
   \includegraphics[width=0.45\textwidth]{DNS_Transpose}
   \caption{Strong scaling result of data reorder and MPI communication; OpenMP is not used.}
   \label{fig:DNS_strong_scale_transpose}
 \end{center}
\end{figure}

% Full timestep performance

\begin{figure}[htb]
 \begin{center}
   \includegraphics[width=0.45\textwidth]{DNS_Parallelism}
   \caption{Comparison of MPI$\times$OpenMP configuration}
   \label{fig:DNS_MPI_OpenMP}
 \end{center}
\end{figure}

\begin{figure}[htb]
 \begin{center}
   \includegraphics[width=0.45\textwidth]{DNS_full_timestep}
   \caption{Strong scaling result of total elapsed time for single timestep; (MPI tasks $\times$ OpenMP threads) is hybrid configuratation which showed  the best performance.}
   \label{fig:DNS_strong_scale_total_elapsed_time}
 \end{center}
\end{figure}





\todo{if we merge this with the previous section, we can have an intro
to dns and then talk about each in detail}

