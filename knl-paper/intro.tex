\section{Introduction}
\label{sec:intro}

Accelerators such as current generation GPGPUs offer relatively high
floating-point throughput and memory bandwidth with a lower relative power
footprint than general-purpose compute platforms~\cite{gpu_hpc:2009}. However,
GPU-based acceleration commonly requires special programming constructs (e.g.
NVIDIA's CUDA language~\cite{nvidia2010programming} or
OpenCL~\cite{opencl_spec}) for the accelerator to work.
Intel's Xeon Phi many-core architectures offer a balance with a smaller core
count than GPGPUs
but an x86 architecture and therefore do not need specialized programming
paradigms.  Existing MPI or OpenMP~\cite{openmp_standard} threaded Fortran, C,
or C++ codes can be compiled and run with relative ease using the Intel compilers.

The applications considered in this paper are taken from existing development
efforts within the PECOS Center (\url{http://pecos.ices.utexas.edu}) at the
University of Texas at Austin.  This
paper reports on two main areas: 1) the level of effort required in porting
software applications from a variety of scientific disciplines written in
commonly used procedural languages; and 2) observed performance of these
applications.  The applications cover Fortran, C, and C++ codes, and include
an example with no prior thread-based parallelism as well as codes with
existing OpenMP or MPI based threading from the following scientific
disciplines: 1) computational fluid dynamics; 2) uncertainty quantification;
3) computational chemistry; and 4) finite element methods.

The remainder of the paper is organized as follows: \S\ref{sec:hardware}
describes the testing infrastructure, \S\ref{sec:cross_compile} describes cross
compiling experiences, \S\ref{sec:apps} presents results of the porting efforts
for each application considered, and \S\ref{sec:summary} summarizes the overall
experiences.

\section{Testing Infrastructure}
\label{sec:hardware}

The test hardware used for this exercise was the recent Stampede cluster
upgrade at the Texas Advanced Computing Center.  This upgrade cluster leaves
the original Stampede cluster hardware untouched and adds compute performance
capabilities with the newest iteration of Intel's MIC architecture codenamed
``Knights Landing (KNL)''.

The ``Knights Landing'' Xeon Phi 7250 compute nodes in the Stampede Upgrade
cluster boast 68 cores, each with 4 hardware threads, are bootable and each run
a lightweight CentOS 7 Linux kernel.  They also have 16GB of high-bandwidth
Multi-Channel Dynamic Random Access Memory (MCDRAM) and 96GB of DDR4-2400.

The login node of the Stampede Upgrade (KNL) cluster is a 14 core (28 thread)
Haswell generation Intel Xeon E5-2695v3 processor with a clock speed of
2.30GHz.  This was used to cross-compile scientific applications for the KNL
compute nodes.  Applications were built with version 17.0.0 20160721 of Intel's
own C, C++, and Fortran compilers.

\section{Third Party Library Cross-Compiling}
\label{sec:cross_compile}

As with many scientific research groups, application development at the PECOS
center employs many open-source libraries.  Fortunately, TACC's module system
built on top of \texttt{lmod} provides many commonly used scientific packages
already compiled with the necessary instructions supported by KNL.  Moreover,
since the Stampede Upgrade cluster contains only bootable KNL nodes, the login
Haswell node is not needed for offloading computation.  We use it only for
compiling and building software.  Compilation is possible on KNL, but it is
much slower.

The following software packages were compiled for the KNL MIC architecture:
Poongback, QUESO, OCCA, ArcSyn3sis, CFOUR.  Building these libraries
for the login node environment is a well supported, common task, but building
them for KNL required cross-compilation.  Many of these libraries utilize a
build system which supports cross-compilation.  Builds that include
KNL-specific instructions were configured by specifying two additional flags
when compiling: \texttt{-xCORE-AVX2 -axMIC-AVX512}.

As an example, the following configure options were used to build version
0.55.0 of the QUESO library for KNL:

{\small
\begin{verbatim}
./configure CC=mpicc CXX=mpicxx  \
   CFLAGS="-xCORE-AVX2 -axMIC-AVX512"  \
   CXXFLAGS="-xCORE-AVX2 -axMIC-AVX512"
\end{verbatim}
}

These strategies successfully built dynamic libraries and executables for KNL.
Where appropriate, the kernels used for benchmarking in the subsequent sections have been 
publicly provided on GitHub: \url{https://github.com/PECOS-KNL/kernels}. 
%\todo{can we comment on if these are actually using avx512 for vectorization?}
